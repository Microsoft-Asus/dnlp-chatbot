from sklearn.model_selection import train_test_split
import numpy as np
import time

EPOCHS = 10
BATCH_SIZE = 32
tf.keras.preprocessing.text.Tokenizer

# vocab_int = ['<PAD>', '<SOS>', '<EOS>', '<OUT>', ...]
